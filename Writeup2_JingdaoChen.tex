\documentclass[twoside,12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,fullpage}
%\usepackage{mymath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}

\begin{document}

\title{CS 8803 DL Writeup Week 2}
\author{Jingdao Chen}
\date{Jan 25}
\maketitle
\openup 1em

\begin{enumerate}
	\item The entropy of the "true" distribution is zero.
	\item Minimizing the cross-entropy is equivalent to minimizing the Kullback-Leibler (KL) divergence.
	\item (b) Softmax classifier
	\item (a) Bias
	\item \begin{enumerate}
		\item The two possible failures that can occur are that the optimization algorithm may not find the correct parameters during training and
				it may learn the wrong function due to overfitting.
		\item The worst case size of the hidden network is exponential to the function dimensionality.
		\item Depth could alleviate this since Montufar \textit{ et al.} (2014) showed that the number of linear regions carved out by a deep linear
				network is exponential in the depth.
		\end{enumerate}
\end{enumerate}

\end{document}
