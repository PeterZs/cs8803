\documentclass[twoside,12pt]{article}
%\usepackage{amsmath,amsfonts,amsthm,fullpage}
%\usepackage{mymath}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{graphicx}
\newcommand{\imsize}{0.5\linewidth}

\begin{document}

\title{CS 8803 DL Assigment 2}
\author{Jingdao Chen}
\date{Feb 24}
\maketitle
\openup 1em

\section{Optimization}

The plots of training and test error for each method is shown below:

\subsection{Stochastic Gradient Descent}

Learning rate = 1e-3

\includegraphics[width=\imsize]{assignment2/results/sgd_train}
\includegraphics[width=\imsize]{assignment2/results/sgd_test}

Learning rate = 1e-2

\includegraphics[width=\imsize]{assignment2/results/sgd_r2_train}
\includegraphics[width=\imsize]{assignment2/results/sgd_r2_test}

Learning rate = 1e-4

\includegraphics[width=\imsize]{assignment2/results/sgd_r4_train}
\includegraphics[width=\imsize]{assignment2/results/sgd_r4_test}
Learning rate decay = 1e-5

\includegraphics[width=\imsize]{assignment2/results/sgd_d5_train}
\includegraphics[width=\imsize]{assignment2/results/sgd_d5_test}

Learning rate decay = 1e-6

\includegraphics[width=\imsize]{assignment2/results/sgd_d6_train}
\includegraphics[width=\imsize]{assignment2/results/sgd_d6_test}

\subsection{SGD + weight decay}

Weight decay = 1e-5

\includegraphics[width=\imsize]{assignment2/results/sgdw_train}
\includegraphics[width=\imsize]{assignment2/results/sgdw_test}

Weight decay = 1e0

\includegraphics[width=\imsize]{assignment2/results/sgdw_w3_train}
\includegraphics[width=\imsize]{assignment2/results/sgdw_w3_test}

Weight decay = 1e-1

\includegraphics[width=\imsize]{assignment2/results/sgdw_w4_train}
\includegraphics[width=\imsize]{assignment2/results/sgdw_w4_test}

\subsection{SGD + weight decay + momentum}

Momentum = 0.1

\includegraphics[width=\imsize]{assignment2/results/sgdwm_train}
\includegraphics[width=\imsize]{assignment2/results/sgdwm_test}

Momentum = 0.5

\includegraphics[width=\imsize]{assignment2/results/sgdwm_m5_train}
\includegraphics[width=\imsize]{assignment2/results/sgdwm_m5_test}

Momentum = 0.9

\includegraphics[width=\imsize]{assignment2/results/sgdwm_m9_train}
\includegraphics[width=\imsize]{assignment2/results/sgdwm_m9_test}

\subsection{Coordinate Descent}

Learning rate = 1e-4

\includegraphics[width=\imsize]{assignment2/results/cd_train}
\includegraphics[width=\imsize]{assignment2/results/cd_test}

Learning rate = 1e-3

\includegraphics[width=\imsize]{assignment2/results/cd_lr3_train}
\includegraphics[width=\imsize]{assignment2/results/cd_lr3_test}

Learning rate = 1e-5

\includegraphics[width=\imsize]{assignment2/results/cd_lr5_train}
\includegraphics[width=\imsize]{assignment2/results/cd_lr5_test}

\subsection{Analysis}

The above tests were conducted over 100 epochs with different gradient descent methods.
Each method differed in terms of convergence rate with respect to their parameters.
For basic stochastic gradient descent, an increase in learning rate increases convergence rate
but leads to more oscillation, whereas learning rate decay causes convergence to slow down
for later iterations.  For stochastic gradient descent with weight decay, the regularization term
allows the optimization to generalize better to test data but oscillation may occur if
the regularization term is set too high. For stochastic gradient descent with momentum, the momentum
term reduces the effect of jumps or oscillations and increases the convergence rate.
For coordinate descent, the convergence rate is slower than that of stochastic gradient descent.
The effect of learning rate is similiar in that a higher learning rate leads to higher convergence rate.

\section{Activation}
The plots of training and test error for each method is shown below:

\subsection{Tanh}
\includegraphics[width=\imsize]{assignment2/results/tanh_train}
\includegraphics[width=\imsize]{assignment2/results/tanh_test}
\subsection{Sigmoid}
\includegraphics[width=\imsize]{assignment2/results/sigmoid_train}
\includegraphics[width=\imsize]{assignment2/results/sigmoid_test}
\subsection{Rectified Linear Unit}
\includegraphics[width=\imsize]{assignment2/results/relu_train}
\includegraphics[width=\imsize]{assignment2/results/relu_test}
\subsection{Rectified Quadratic Unit}
\includegraphics[width=\imsize]{assignment2/results/requ_train}
\includegraphics[width=\imsize]{assignment2/results/requ_test}

\subsection{Comparison}

The activation functions are plotted below. The graphical comparison shows that tanh is similiar in shape with sigmoid
but with a range between -1 and 1. On the other hand, ReLU and ReQU are similiar in that negative values are set to zero
but ReQU uses a quadratic instead of linear function for positive values.

\includegraphics[width=\imsize]{assignment2/MyTanh}

\includegraphics[width=\imsize]{assignment2/MySigmoid}

\includegraphics[width=\imsize]{assignment2/MyReLU}

\includegraphics[width=\imsize]{assignment2/MyReQU}


From the plots of training and test error, tanh showed fast convergence rate whereas sigmoid demonstrated
slow convergence at earlier iterations. The Rectified Linear and Quadratic Unit also showed slightly faster
convergence rate compared to tanh. A major difference between sigmoid (or tanh) and ReLU (ReQU) is that
the former plateaus at later iterations whereas the latter continues to improve.

\end{document}
